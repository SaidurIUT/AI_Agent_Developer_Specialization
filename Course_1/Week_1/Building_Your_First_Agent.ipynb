{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Simple AI Agent, Part 1"
      ],
      "metadata": {
        "id": "hgeXPNJ5wTew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you understand the agent loop and how to craft effective prompts, we can build a simple AI agent. This agent will be able to list files in a directory, read their content, and answer questions about them. We’ll break down the agent loop—how it receives input, decides on actions, executes them, and updates its memory—step by step.\n",
        "\n"
      ],
      "metadata": {
        "id": "rq2Y3IoNwaEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Agent Loop in Python"
      ],
      "metadata": {
        "id": "MfLtZEJvwy10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent loop is the backbone of our AI agent, enabling it to perform tasks by combining response generation, action execution, and memory updates in an iterative process. This section focuses on how the agent loop works and its role in making the agent dynamic and adaptive.\n",
        "\n",
        "1. **Construct Prompt:** Combine the agent’s memory, user input, and system rules into a single prompt. This ensures the LLM has all the context it needs to decide on the next action, maintaining continuity across iterations.\n",
        "\n",
        "2. **Generate Response:** Send the constructed prompt to the LLM and retrieve a response. This response will guide the agent’s next step by providing instructions in a structured format.\n",
        "\n",
        "3. **Parse Response:** Extract the intended action and its parameters from the LLM’s output. The response must adhere to a predefined structure (e.g., JSON format) to ensure it can be interpreted correctly.\n",
        "\n",
        "4. **Execute Action:** Use the extracted action and its parameters to perform the requested task with the appropriate tool. This could involve listing files, reading content, or printing a message.\n",
        "\n",
        "5. **Convert Result to String:** Format the result of the executed action into a string. This allows the agent to store the result in its memory and provide clear feedback to the user or itself.\n",
        "\n",
        "6. **Continue Loop?:** Evaluate whether the loop should continue based on the current action and results. The loop may terminate if a “terminate” action is specified or if the agent has completed the task.\n",
        "\n",
        "The agent iterates through this loop, refining its behavior and adapting its actions until it reaches a stopping condition. This process is what enables the agent to interact dynamically and respond intelligently to tasks.\n",
        "\n",
        "Here’s how these steps come together in code:"
      ],
      "metadata": {
        "id": "9ZWGgkYDw8wP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install litellm"
      ],
      "metadata": {
        "id": "9ULPy1cU7xVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RjMA4dHA7lQa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"groq/llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return response.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "Wv2tn13i74VP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "532a0174"
      },
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def extract_markdown_block(text, block_name):\n",
        "    \"\"\"Extracts a markdown block from a string.\"\"\"\n",
        "    pattern = rf\"```{block_name}\\n(.*?)\\n```\"\n",
        "    match = re.search(pattern, text, re.DOTALL)\n",
        "    if match:\n",
        "        return match.group(1).strip()\n",
        "    return None\n",
        "\n",
        "def list_files():\n",
        "    \"\"\"Lists all files in the current directory.\"\"\"\n",
        "    return os.listdir('.')\n",
        "\n",
        "def read_file(file_name):\n",
        "    \"\"\"Reads the content of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_name, 'r') as f:\n",
        "            return f.read()\n",
        "    except FileNotFoundError:\n",
        "        return f\"Error: File '{file_name}' not found.\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5909bdaa"
      },
      "source": [
        "# Initialize variables for the agent loop\n",
        "iterations = 0\n",
        "max_iterations = 5 # Set a maximum number of iterations to prevent infinite loops"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_action(response: str) -> Dict:\n",
        "    \"\"\"Parse the LLM response into a structured action dictionary.\"\"\"\n",
        "    try:\n",
        "        response = extract_markdown_block(response, \"action\")\n",
        "        response_json = json.loads(response)\n",
        "        if \"tool_name\" in response_json and \"args\" in response_json:\n",
        "            return response_json\n",
        "        else:\n",
        "            return {\"tool_name\": \"error\", \"args\": {\"message\": \"You must respond with a JSON tool invocation.\"}}\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"tool_name\": \"error\", \"args\": {\"message\": \"Invalid JSON response. You must respond with a JSON tool invocation.\"}}"
      ],
      "metadata": {
        "id": "Q0QiBl2A2Pks"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_rules = [{\n",
        "    \"role\": \"system\",\n",
        "    \"content\": \"\"\"\n",
        "You are an AI agent that can perform tasks by using available tools.\n",
        "\n",
        "Available tools:\n",
        "- list_files() -> List[str]: List all files in the current directory.\n",
        "- read_file(file_name: str) -> str: Read the content of a file.\n",
        "- terminate(message: str): End the agent loop and print a summary to the user.\n",
        "\n",
        "If a user asks about files, list them before reading.\n",
        "\n",
        "Every response MUST have an action.\n",
        "Respond in this format:\n",
        "\n",
        "\\\\`\\\\`\\\\`action\n",
        "{\n",
        "    \"tool_name\": \"insert tool_name\",\n",
        "    \"args\": {...fill in any required arguments here...}\n",
        "}\n",
        "\\\\`\\\\`\\\\`\n",
        "\"\"\"\n",
        "}]"
      ],
      "metadata": {
        "id": "OhxialiI14Y7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = [\n",
        "    {\"role\": \"user\", \"content\": \"What files are in this directory?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"```action\\n{\\\"tool_name\\\":\\\"list_files\\\",\\\"args\\\":{}}\\n```\"},\n",
        "    {\"role\": \"user\", \"content\": \"[\\\"file1.txt\\\", \\\"file2.txt\\\"]\"}\n",
        "]"
      ],
      "metadata": {
        "id": "HezI0UTk2GI5"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Agent Loop\n",
        "while iterations < max_iterations:\n",
        "\n",
        "    # 1. Construct prompt: Combine agent rules with memory\n",
        "    prompt = agent_rules + memory\n",
        "\n",
        "    # 2. Generate response from LLM\n",
        "    print(\"Agent thinking...\")\n",
        "    response = generate_response(prompt)\n",
        "    print(f\"Agent response: {response}\")\n",
        "\n",
        "    # 3. Parse response to determine action\n",
        "    action = parse_action(response)\n",
        "\n",
        "    result = \"Action executed\"\n",
        "\n",
        "    if action[\"tool_name\"] == \"list_files\":\n",
        "        result = {\"result\":list_files()}\n",
        "    elif action[\"tool_name\"] == \"read_file\":\n",
        "        result = {\"result\":read_file(action[\"args\"][\"file_name\"])}\n",
        "    elif action[\"tool_name\"] == \"error\":\n",
        "        result = {\"error\":action[\"args\"][\"message\"]}\n",
        "    elif action[\"tool_name\"] == \"terminate\":\n",
        "        print(action[\"args\"][\"message\"])\n",
        "        break\n",
        "    else:\n",
        "        result = {\"error\":\"Unknown action: \"+action[\"tool_name\"]}\n",
        "\n",
        "    print(f\"Action result: {result}\")\n",
        "\n",
        "    # 5. Update memory with response and results\n",
        "    memory.extend([\n",
        "        {\"role\": \"assistant\", \"content\": response},\n",
        "        {\"role\": \"user\", \"content\": json.dumps(result)}\n",
        "    ])\n",
        "\n",
        "    # 6. Check termination condition\n",
        "    if action[\"tool_name\"] == \"terminate\":\n",
        "        break\n",
        "\n",
        "    iterations += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "-cQFGUeE8AjF",
        "outputId": "b4c4c73a-9043-47e0-fbd5-ebe6e2c1a03d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent thinking...\n",
            "Agent response: I have listed the files in the directory. Now, I would like to ask if you would like to read one of these files.\n",
            "\n",
            "Please respond with the file name you would like to read.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/main.py:463: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 9 fields but got 5: Expected `Message` - serialized value may not be as expected [input_value=Message(content='I have l...er_specific_fields=None), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [input_value=Choices(finish_reason='st...r_specific_fields=None)), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "the JSON object must be str, bytes or bytearray, not NoneType",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-2336274795.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# 3. Parse response to determine action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Action executed\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-7-2551392467.py\u001b[0m in \u001b[0;36mparse_action\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_markdown_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mresponse_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"tool_name\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"args\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m             raise TypeError(f'the JSON object must be str, bytes or bytearray, '\n\u001b[0m\u001b[1;32m    340\u001b[0m                             f'not {s.__class__.__name__}')\n\u001b[1;32m    341\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetect_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'surrogatepass'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not NoneType"
          ]
        }
      ]
    }
  ]
}