{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BBSV4hg8gh34",
        "CUxYPYBFhEjf",
        "2tabbpM4dvUv",
        "nd1l2OxVhVqa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Running the Code Samples in the Course"
      ],
      "metadata": {
        "id": "BBSV4hg8gh34"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thoughout the course, we will be using\n",
        "LiteLLM\n",
        ", a library that provides a unified interface for interacting with over 100 different LLMs including GPT-4, Claude, Gemini, and many others. By using a single interface, we can easily switch between different models without changing our code, making it perfect for testing different LLMs or having fallback options in production. This will also allow us to make our agent reusable across LLMs — although we may need to do some prompt engineering to make it work with a new LLM.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpbq9Z8Bgjrs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "sJJpT4UGk99E"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "api_key = userdata.get('GROQ_API_KEY')\n",
        "os.environ['GROQ_API_KEY'] = api_key"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programmatic Prompting for Agents"
      ],
      "metadata": {
        "id": "CUxYPYBFhEjf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sending Prompts Programmatically & Managing Memory\n",
        "**Note: a link to the complete code in a Google Colab notebook will be provided in the next item in the course. Read through this lesson and then click next to get the link to the code.**\n",
        "\n",
        "To get started building agents, we need to understand how to send prompts to LLMs. Agents require two key capabilities:\n",
        "\n",
        "1. **Programmatic prompting** - Automating the prompt-response cycle that humans do manually in a conversation. This forms the foundation of the Agent Loop we’ll explore.\n",
        "\n",
        "2. **Memory managemen**t - Controlling what information persists between iterations, like API calls and their results, to maintain context through the agent’s decision-making process.\n",
        "\n",
        "Programmatically sending prompts is how we move from having a human type in prompts and then take action based on the LLM’s response to having an agent that can do this automatically. The Agent Loop that we will begin building over the next several readings will be programmatically sending prompts to the LLM and then taking action based on the LLM’s response.\n",
        "\n",
        "We will also need to understand how to manage what the LLM knows or remembers. This is important because we want to be able to control what information the LLM has in each iteration of the loop. For example, if it just called an API, we want it to remember what API it asked to be invoked and what the result of that action was."
      ],
      "metadata": {
        "id": "VOLuCl7yhGlO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Usage\n",
        "Here’s a simple example of how to send prompts to an LLM using LiteLLM:"
      ],
      "metadata": {
        "id": "euz2T9K0ha0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install litellm"
      ],
      "metadata": {
        "id": "ZqNkDy_zoayr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c12bbd09",
        "collapsed": true
      },
      "source": [
        "from litellm import completion\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def generate_response(messages: List[Dict]) -> str:\n",
        "    \"\"\"Call LLM to get response\"\"\"\n",
        "    response = completion(\n",
        "        model=\"groq/llama3-8b-8192\",\n",
        "        messages=messages,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s break down the key components:\n",
        "\n",
        "1. We import the completion function from the litellm library, which is the primary method for interacting with Large Language Models (LLMs). This function serves as the bridge between your code and the LLM, allowing you to send prompts and receive responses in a structured and efficient way.\n",
        "\n",
        "  How completion Works:\n",
        "\n",
        "  * Input: You provide a prompt, which is a list of messages that you want the model to process. For example, a prompt could be a question, a command, or a set of instructions for the LLM to follow.\n",
        "  * Output: The completion function returns the model’s response, typically in the form of generated text based on your prompt.\n",
        "2. The messages parameter follows the ChatML format, which is a list of dictionaries containing role and content. The role attribute indicates who is “speaking” in the conversation. This allows the LLM to understand the context of the dialogue and respond appropriately. The roles include:\n",
        "\n",
        "  * “system”: Provides the model with initial instructions, rules, or configuration for how it should behave throughout the session. This message is not part of the “conversation” but sets the ground rules or context (e.g., “You will respond in JSON.”).\n",
        "  * “user”: Represents input from the user. This is where you provide your prompts, questions, or instructions.\n",
        "  * “assistant”: Represents responses from the AI model. You can include this role to provide context for a conversation that has already started or to guide the model by showing sample responses. These messages are interpreted as what the “model” said in the passt.\n",
        "3. We specify the model using the provider/model format (e.g., “openai/gpt-4o”)\n",
        "\n",
        "4. The response contains the generated text in choices[0].message.content. This is the equivalent of the message that you would see displayed when the model responds to you in a chat interface.\n",
        "\n"
      ],
      "metadata": {
        "id": "pTVulN8Kh0Kz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick Exercise**\n",
        "\n",
        "As a practice exercise, try creating a prompt that only provides the response as a Base64 encoded string and refuses to answer in natural language. Can you get your LLM to only respond in Base64?"
      ],
      "metadata": {
        "id": "hWL-_AchiTEU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ad1c731",
        "collapsed": true
      },
      "source": [
        "import base64\n",
        "\n",
        "messages_base64 = [\n",
        "    {\"role\": \"system\", \"content\": \"Your only task is to respond to the user's query by first encoding your answer as a Base64 string. You MUST only provide the Base64 encoded string as your response and should NOT include any other text, explanations, or natural language.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Provide a short summary of the process of photosynthesis.\"}\n",
        "]\n",
        "\n",
        "response_base64 = generate_response(messages_base64)\n",
        "\n",
        "print(\"Raw response 1:\")\n",
        "print(response_base64)\n",
        "\n",
        "# Attempt to decode the response to see if it worked\n",
        "try:\n",
        "    decoded_response = base64.b64decode(response_base64).decode('utf-8')\n",
        "    print(\"Decoded response:\")\n",
        "    print(decoded_response)\n",
        "except Exception as e:\n",
        "    print(\"Could not decode the response as Base64. The LLM may not have followed the instructions.\")\n",
        "    print(\"Raw response 2:\")\n",
        "    print(response_base64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "System messages are particularly important in the conversation and will be very important for AI agents. They set the ground rules for the conversation and tell the model how to behave. Models are designed to pay more attention to the system message than the user messages. We can “program” the AI agent through system messages.\n",
        "\n",
        "Let’s simulate a customer service interaction for a customer service agent that always tells the customer to turn off their computer or modem with system messages:"
      ],
      "metadata": {
        "id": "riJexfb-if69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": \"How do I get pet back home.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tOSBkxCN1Ez4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The system message is the most important part of this prompt. It tells the model how to behave. The user message is the question that we want the model to answer. The system instructions lay the ground rules for the interaction."
      ],
      "metadata": {
        "id": "RiW1ErdGi6VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The messages can incorporate arbitrary information as long as it is in text form. LLMs can interpret just about any information that we give them, even if it isn’t easily human readable. Let’s generate an implementation of a function based on some information in a dictionary:"
      ],
      "metadata": {
        "id": "8AXQlXQFi-EN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "code_spec = {\n",
        "    'name': 'swap_keys_values',\n",
        "    'description': 'Swaps the keys and values in a given dictionary.',\n",
        "    'params': {\n",
        "        'd': 'A dictionary with unique values.'\n",
        "    },\n",
        "}\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\",\n",
        "     \"content\": \"You are an expert software engineer that writes clean functional code. You always document your functions.\"},\n",
        "    {\"role\": \"user\", \"content\": f\"Please implement: {json.dumps(code_spec)}\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "id": "ZkZVOs876jJT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will rely heavily on the ability to send the LLM just about any type of information, particularly JSON, when we start building agents. This is a simple example of how we can use JSON to send information to the LLM, but you can see how we could provide it JSON with information about the result of an API call, for example."
      ],
      "metadata": {
        "id": "zoDXApUCjBUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take input from user"
      ],
      "metadata": {
        "id": "nUy5mB-VjLSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "what_to_help_with = input(\"What do you need help with?\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful customer service representative. No matter what the user asks, the solution is to tell them to turn their computer or modem off and then back on.\"},\n",
        "    {\"role\": \"user\", \"content\": what_to_help_with}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CVPQB6d6dOW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Giving Agents Memory\n"
      ],
      "metadata": {
        "id": "2tabbpM4dvUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLMs Do Not Have Memory\n",
        "\n",
        "When we are building an Agent, we need it to remember its actions and the result of those actions. For example, if it tries to create a calendar event for a meeting and the API call fails due to an incorrect parameter value that it provided, we want it to remember that the API call failed and why. This way, it can correct the mistake and try again. If we have a complex task that we break down into multiple steps, we need the Agent to remember the results of each step to ensure that it can continue the task from where it left off. Memory is crucial for Agents.\n",
        "\n",
        "### LLMs Do Not Have Memory\n",
        "\n",
        "When interacting with an LLM, the model does not inherently “remember” previous conversations or responses. Every time you call the model, it generates a response based solely on the information provided in the messages parameter. If previous context is not included in the messages, the model will not have any knowledge of it.\n",
        "\n",
        "This means that to simulate continuity in a conversation, you must explicitly pass all relevant prior messages (including system, user, and assistant roles) in the messages list for each request.\n",
        "\n",
        "**Example 1: Missing Context in the Prompt**\n",
        "\n"
      ],
      "metadata": {
        "id": "lPC-NHb4gCmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "\n",
        "# Second query without including the previous response\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NgD_ojEUdPgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** In the second request, the model doesn’t “remember” the function it wrote in the first interaction. Since the information is not included in the second prompt, the model cannot connect the two."
      ],
      "metadata": {
        "id": "fchLBp2JdoxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2: Including Previous Responses for Continuity**\n",
        "\n",
        "To fix this issue, we need to add new messages with the “assistant” role to the messages list with the content of the prior response from the LLM. This way, the model can see what code it wrote previously and can build on that."
      ],
      "metadata": {
        "id": "7ysZ9HhceKQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)\n",
        "\n",
        "# We are going to make this verbose so it is clear what\n",
        "# is going on. In a real application, you would likely\n",
        "# just append to the messages list.\n",
        "messages = [\n",
        "   {\"role\": \"system\", \"content\": \"You are an expert software engineer that prefers functional programming.\"},\n",
        "   {\"role\": \"user\", \"content\": \"Write a function to swap the keys and values in a dictionary.\"},\n",
        "\n",
        "   # Here is the assistant's response from the previous step\n",
        "   # with the code. This gives it \"memory\" of the previous\n",
        "   # interaction.\n",
        "   {\"role\": \"assistant\", \"content\": response},\n",
        "\n",
        "   # Now, we can ask the assistant to update the function\n",
        "   {\"role\": \"user\", \"content\": \"Update the function to include documentation.\"}\n",
        "]\n",
        "\n",
        "response = generate_response(messages)\n",
        "print(response)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "suzm-HQteOxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:** By including the assistant’s previous response in the messages, the model can maintain context and provide an appropriate response to the follow-up question.\n",
        "\n",
        "**Key Takeaways**\n",
        "\n",
        "1. **No Inherent Memory:** The LLM has no knowledge of past interactions unless explicitly provided in the current prompt (via messages).\n",
        "2. **Provide Full Context:** To simulate continuity in a conversation, include all relevant messages (both user and assistant responses) in the messages parameter.\n",
        "3. **Role of Assistant Messages:** Adding previous responses as assistant messages allows the model to maintain a coherent conversation and build on earlier exchanges. For an agent, this will allow it to remember what actions, such as API calls, it took in the past.\n",
        "4. **Memory Management:** We can control what the LLM remembers or does not remember by managing what messages go into the conversation. Causing the LLM to forget things can be a powerful tool in some circumstances, such as when we need to break a pattern of poor responses from an Agent.\n",
        "\n",
        "**Why This Matters**\n",
        "\n",
        "Understanding the stateless nature of LLMs is crucial for designing agents that rely on multi-turn conversations with their environment. Developers must explicitly manage and provide context to ensure the model generates accurate and relevant responses."
      ],
      "metadata": {
        "id": "o68he7__fHfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practicing Programmatic Prompting for Agents"
      ],
      "metadata": {
        "id": "78ME4_01mu1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a Quasi-Agent\n",
        "\n",
        "For practice, we are going to write a quasi-agent that can write Python functions based on user requirements. It isn’t quite a real agent, it can’t react and adapt, but it can do something useful for us.\n",
        "\n",
        "The quasi-agent will ask the user what they want code for, write the code for the function, add documentation, and finally include test cases using the unittest framework. This exercise will help you understand how to maintain context across multiple prompts and manage the information flow between the user and the LLM. It will also help you understand the pain of trying to parse and handle the output of an LLM that is not always consistent.\n",
        "\n"
      ],
      "metadata": {
        "id": "FZXevGB-mxUi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Exercise"
      ],
      "metadata": {
        "id": "nd1l2OxVhVqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise will allow you to practice programmatically sending prompts to an LLM and managing memory.\n",
        "\n",
        "For this exercise, you should write a program that uses sequential prompts to generate any Python function based on user input. The program should:\n",
        "\n",
        "1. First Prompt:\n",
        "\n",
        "  * Ask the user what function they want to create\n",
        "  * Ask the LLM to write a basic Python function based on the user’s description\n",
        "  * Store the response for use in subsequent prompts\n",
        "  * Parse the response to separate the code from the commentary by the LLM\n",
        "\n",
        "2. Second Prompt:\n",
        "\n",
        "  * Pass the code generated from the first prompt\n",
        "  * Ask the LLM to add comprehensive documentation including:\n",
        "    * Function description\n",
        "    * Parameter descriptions\n",
        "    * Return value description\n",
        "    * Example usage\n",
        "    * Edge cases\n",
        "3. Third Prompt:\n",
        "\n",
        "  * Pass the documented code generated from the second prompt\n",
        "  * Ask the LLM to add test cases using Python’s unittest framework\n",
        "  * Tests should cover:\n",
        "    * Basic functionality\n",
        "    * Edge cases\n",
        "    * Error cases\n",
        "    * Various input scenarios\n",
        "Requirements:\n",
        "\n",
        "* Use the LiteLLM library\n",
        "* Maintain conversation context between prompts\n",
        "* Print each step of the development process\n",
        "* Save the final version to a Python file\n",
        "\n",
        "If you want to practice further, try using the system message to force the LLM to always output code that has a specific style or uses particular libraries."
      ],
      "metadata": {
        "id": "Ds2-bbbPm5nq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24aedd7"
      },
      "source": [
        "### Task Solution\n",
        "Create a Python function based on a user-provided description, add documentation and unit tests, and save the combined code to a file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bdd52f1"
      },
      "source": [
        "#### Get user input for the function description\n",
        "\n",
        "##### Subtask:\n",
        "Prompt the user to describe the Python function they want to create.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77b21a1f"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to prompt the user for a function description and store it in a variable. The provided code block already does this using the `input()` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc079b98"
      },
      "source": [
        "import re\n",
        "\n",
        "def extract_code_block(response: str) -> str:\n",
        "    \"\"\"Extract code block from response\"\"\"\n",
        "    if not '```' in response:\n",
        "        return response\n",
        "\n",
        "    code_block = response.split('```')[1].strip()\n",
        "    if code_block.startswith(\"python\"):\n",
        "        code_block = code_block[6:]\n",
        "\n",
        "    return code_block"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The LLM often includes commentary with its code. This function extracts just the code block, making it easier to build upon in subsequent prompts."
      ],
      "metadata": {
        "id": "_QhZHobIiB6J"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62113851"
      },
      "source": [
        "function_description = input(\"What function do you want to create?\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a Python expert helping to develop a function.\"}\n",
        "]\n",
        "\n",
        "messages.append({\n",
        "    \"role\": \"user\",\n",
        "    \"content\": f\"Write a Python function that {function_description}. Output the function in a ```python code block```.\"\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49d695d8"
      },
      "source": [
        "#### Generate initial function code\n",
        "\n",
        "##### Subtask:\n",
        "Use LiteLLM with a system message to generate the initial Python function code based on the user's description. Extract the code block from the LLM's response.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35391a19"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the generate_response function with the messages list to get the initial code, then extract and print the code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6433b7c7"
      },
      "source": [
        "initial_response = generate_response(messages)\n",
        "print(\"Initial response:\")\n",
        "print(initial_response)\n",
        "\n",
        "initial_code = extract_code_block(initial_response)\n",
        "print(\"\\nExtracted initial code:\")\n",
        "print(initial_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90ec8d00"
      },
      "source": [
        "#### Generate documented code\n",
        "\n",
        "##### Subtask:\n",
        "Use LiteLLM with an updated system message and the previously generated code to add comprehensive documentation to the function. Extract the documented code block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "293320d4"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a new list of messages for the second prompt, including the previous conversation context and the instruction to add comprehensive documentation. Then, call the generate_response function to get the documented code and extract the code block using the extract_code_block function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77a4285d"
      },
      "source": [
        "messages_documented = [\n",
        "   {\"role\": \"system\", \"content\": \"You are a Python expert helping to develop a function.\"},\n",
        "   {\"role\": \"user\", \"content\": f\"Write a Python function that {function_description}. Output the function in a ```python code block```.\"},\n",
        "   {\"role\": \"assistant\", \"content\": initial_response},\n",
        "   {\"role\": \"user\", \"content\": \"\"\"Update the following Python function to include comprehensive documentation. The documentation should include:\n",
        "- Function description\n",
        "- Parameter descriptions\n",
        "- Return value description\n",
        "- Example usage\n",
        "- Edge cases\n",
        "Output the documented function in a ```python code block```.\"\"\"}\n",
        "]\n",
        "\n",
        "documented_response = generate_response(messages_documented)\n",
        "print(\"Documented response:\")\n",
        "print(documented_response)\n",
        "\n",
        "documented_code = extract_code_block(documented_response)\n",
        "print(\"\\nExtracted documented code:\")\n",
        "print(documented_code)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3662c3f"
      },
      "source": [
        "#### Generate test cases\n",
        "\n",
        "##### Subtask:\n",
        "Use LiteLLM with another updated system message and the documented code to generate unit tests for the function using the `unittest` framework. Extract the test code block.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3837551"
      },
      "source": [
        "**Reasoning**:\n",
        "Use LiteLLM with another updated system message and the documented code to generate unit tests for the function using the `unittest` framework, and extract the test code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c56a3f06"
      },
      "source": [
        "messages_tested = [\n",
        "   {\"role\": \"system\", \"content\": \"You are a Python expert helping to develop a function.\"},\n",
        "   {\"role\": \"user\", \"content\": f\"Write a Python function that {function_description}. Output the function in a ```python code block```.\"},\n",
        "   {\"role\": \"assistant\", \"content\": initial_response},\n",
        "   {\"role\": \"user\", \"content\": \"\"\"Update the following Python function to include comprehensive documentation. The documentation should include:\n",
        "- Function description\n",
        "- Parameter descriptions\n",
        "- Return value description\n",
        "- Example usage\n",
        "- Edge cases\n",
        "Output the documented function in a ```python code block```.\"\"\"},\n",
        "    {\"role\": \"assistant\", \"content\": documented_response},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"Write unit tests for the following Python function using the `unittest` framework. The tests should cover:\n",
        "- Basic functionality\n",
        "- Edge cases\n",
        "- Error cases\n",
        "- Various input scenarios\n",
        "Output the test code in a ```python code block```.\"\"\"}\n",
        "]\n",
        "\n",
        "tested_response = generate_response(messages_tested)\n",
        "print(\"Tested response:\")\n",
        "print(tested_response)\n",
        "\n",
        "test_code = extract_code_block(tested_response)\n",
        "print(\"\\nExtracted test code:\")\n",
        "print(test_code)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47f75d4a"
      },
      "source": [
        "#### Combine code and tests\n",
        "\n",
        "##### Subtask:\n",
        "Combine the documented function code and the generated test cases into a single string.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86d59463"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the documented function code and the generated test cases into a single string as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bba731fb"
      },
      "source": [
        "combined_code = documented_code + \"\\n\\n\" + test_code\n",
        "print(\"Combined code:\")\n",
        "print(combined_code)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6a4e729"
      },
      "source": [
        "#### Save to file\n",
        "\n",
        "##### Subtask:\n",
        "Save the combined code to a Python file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c2a31a"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the combined code to a Python file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "854e4043"
      },
      "source": [
        "file_name = \"my_function_with_tests.py\"\n",
        "with open(file_name, 'w') as f:\n",
        "    f.write(combined_code)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adcb6921"
      },
      "source": [
        "#### Summary:\n",
        "\n",
        "##### Data Analysis Key Findings\n",
        "\n",
        "*   The user's request for a Python function was successfully captured using the `input()` function.\n",
        "*   LiteLLM was effectively used in multiple steps to generate the initial function code, add comprehensive documentation (including description, parameters, return value, examples, and edge cases), and create unit tests using the `unittest` framework (covering basic functionality, edge cases, error cases, and various input scenarios).\n",
        "*   A custom `extract_code_block` function successfully extracted the Python code from the LLM responses at each stage.\n",
        "*   The documented function code and the generated test code were successfully combined into a single string.\n",
        "*   The combined code was successfully saved to a Python file named \"my\\_function\\_with\\_tests.py\".\n",
        "\n",
        "##### Insights or Next Steps\n",
        "\n",
        "*   The process demonstrates a robust workflow for using an LLM to generate, document, and test Python functions based on a user description.\n",
        "*   The generated Python file (\"my\\_function\\_with\\_tests.py\") is now ready to be executed to verify the function's behavior and the correctness of the tests.\n"
      ]
    }
  ]
}